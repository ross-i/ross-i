{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ICD-10 Prediction using Pre-Trained BERT Model\n",
    "This script trains a [BertForSequenceClassification model]{https://huggingface.co/microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext} to predict the ICD-10 code given the text of a cancer pathology report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset as torch_dataset\n",
    "from torch import optim\n",
    "from transformers import AutoTokenizer, BertForSequenceClassification\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(fn, Xcol=\"c.path_notes\", ycol='c.icd10_after_spilt'):\n",
    "    data = pd.read_csv(fn, sep='\\t')\n",
    "    X = data[Xcol].values\n",
    "    labels = data[ycol].values\n",
    "    labels = labels.reshape(-1, 1) #shape as 2d array to feed to OneHotEncoder\n",
    "    return X, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(\"dr\\.\",'dr', text)\n",
    "    text = re.sub('m\\.d\\.', 'md', text)\n",
    "    text = re.sub('a\\.m\\.','am', text)\n",
    "    text = re.sub('p\\.m\\.','pm', text)\n",
    "    text = re.sub(\"\\d+\\.\\d+\", 'floattoken', text)\n",
    "    text = re.sub(\"\\.{2,}\", '.', text)\n",
    "    text = re.sub('[^\\w_|\\.|\\?|!]+', ' ', text)\n",
    "    text = re.sub('\\.', ' .', text)\n",
    "    text = re.sub('\\?', ' ? ', text)\n",
    "    text = re.sub('!', ' ! ', text)\n",
    "    text = re.sub('\\d{3,}', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(data, tokenizer, max_length=100):\n",
    "    input_ids = []\n",
    "    attention_mask = []\n",
    "    for text in data:\n",
    "        tokenized_text = tokenizer.encode_plus(\n",
    "            process(text),\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_length,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        \n",
    "        input_ids.append(tokenized_text['input_ids'])\n",
    "        attention_mask.append(tokenized_text['attention_mask'])\n",
    "    \n",
    "    return torch.tensor(input_ids, dtype=torch.long), torch.tensor(attention_mask, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch_dataset):\n",
    "    def __init__(self, ii, am, y):\n",
    "        self.ii=ii\n",
    "        self.am=am\n",
    "        self.y=y #this is very important\n",
    "    def __len__(self):\n",
    "        #return the number of data points\n",
    "        return self.ii.shape[0]\n",
    "    def __getitem__(self, idx):        \n",
    "        # use the notation DatasetName[idx]\n",
    "        # to get a data point (x,y) by idx\n",
    "        # we need to convert numpy array to torch tensor\n",
    "        ii=self.ii[idx] # cast as tensor\n",
    "        am=self.am[idx]\n",
    "        y=self.y[idx]\n",
    "        return ii, am, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, dataloader, epoch, device):    \n",
    "    model.train() #set model to train mode\n",
    "    loss_train=0\n",
    "    for batch_idx, (ii, am, y) in enumerate(dataloader):\n",
    "        ii, am, y = ii.to(device), am.to(device), y.to(device)\n",
    "        optimizer.zero_grad() #clear the grad of each parameter\n",
    "        output = model(ii, am, labels=y) #forward pass\n",
    "        loss = output.loss \n",
    "        loss.backward() #backward pass\n",
    "        optimizer.step() #update parameters\n",
    "        loss_train+=loss.item()\n",
    "        if batch_idx % 60 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * ii.size(0), len(dataloader.dataset),\n",
    "                    100. * batch_idx / len(dataloader), loss.item()))\n",
    "    loss_train/=len(dataloader)\n",
    "    return loss_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(model, dataloader, device):\n",
    "    model.eval()\n",
    "    f1s = []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (ii, am, y) in enumerate(dataloader):\n",
    "            ii, am = ii.to(device), am.to(device)\n",
    "            logits = model(ii, am).logits #forward pass\n",
    "            y_pred = np.argmax(logits, axis=1)\n",
    "            y_true = np.argmax(y, axis=1)\n",
    "            f1s.append(f1_score(y_true, y_pred, average='micro'))\n",
    "    return np.mean(f1s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(epochs=10, max_datapoints=None, max_seq_length=100, shuffle=True):\n",
    "    \n",
    "    X, labels = load_data(\"icd10sitesonly.txt\")\n",
    "    \n",
    "    if shuffle:\n",
    "        X, labels = shuffle(X, labels)\n",
    "    \n",
    "    ohe = OneHotEncoder(sparse=False)\n",
    "    y = torch.tensor(ohe.fit_transform(labels), dtype=torch.float32)\n",
    "    \n",
    "    if max_datapoints is not None:\n",
    "        X = X[:max_datapoints]\n",
    "        y = y[:max_datapoints]\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\")\n",
    "\n",
    "    input_ids, attn_mask = encode(X, tokenizer, max_length=max_seq_length)\n",
    "\n",
    "    ii_train, ii_test, am_train, am_test, y_train, y_test = train_test_split(input_ids, attn_mask, y, test_size=0.1)\n",
    "    ii_train, ii_val, am_train, am_val, y_train, y_val = train_test_split(ii_train, am_train, y_train, test_size=0.2)\n",
    "\n",
    "    train_set = MyDataset(ii_train, am_train, y_train)\n",
    "    val_set = MyDataset(ii_val, am_val, y_val)\n",
    "    test_set = MyDataset(ii_test, am_test, y_test)\n",
    "\n",
    "    dataloader_train = DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "    dataloader_val = DataLoader(val_set, batch_size=64, shuffle=False)\n",
    "    dataloader_test = DataLoader(test_set, batch_size=64, shuffle=False)\n",
    "\n",
    "    model = BertForSequenceClassification.from_pretrained(\n",
    "        \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\",\n",
    "        num_labels=y.shape[1]\n",
    "    )\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer,step_size=70, gamma=0.5)\n",
    "\n",
    "    loss_train_list=[]\n",
    "    acc_val_list=[]\n",
    "    for epoch in range(epochs):\n",
    "        #print(\"Current learning rate:\", scheduler.get_last_lr())\n",
    "        loss_train = train(model, optimizer, dataloader_train, epoch, device)\n",
    "        loss_train_list.append(loss_train)\n",
    "        print('epoch', epoch, 'training loss:', loss_train)\n",
    "        acc = score(model, dataloader_val, device)\n",
    "        acc_val_list.append(acc)\n",
    "        print('epoch', epoch, 'validation accuracy:', acc)\n",
    "        scheduler.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert",
   "language": "python",
   "name": "bert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
